---
permalink: /
title: "🤖 Yaxin Luo - AI Researcher"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
classes: wide tech-decoration
---

<div class="research-section" data-parallax="0.3">

## 🧠 About Me

Greetings! I'm a **First-Year Machine Learning PhD student** at [**MBZUAI**](https://mbzuai.ac.ae/), where I'm privileged to work under the guidance of an exceptional advisory team: [**Prof. Zhiqiang Shen**](https://zhiqiangshen.com/), [**Dr. Fabio Pizzati**](https://fabvio.github.io/), and [**Prof. Ivan Laptev**](https://www.di.ens.fr/~laptev/). I'm also collaborating closely with my research partner [**Xiaofu Chen**](https://xxfchen.github.io/XiaofuChen/).

My academic journey began with a **Bachelor's degree from [Technical University of Denmark](https://www.dtu.dk/english/)**, where I was mentored by [**Prof. Dim P. Papadopoulos**](https://dimipapa.github.io/). Prior to my PhD, I gained valuable experience as a **Research Assistant at MBZUAI**.

</div>

<div class="research-section" data-parallax="0.2">

## 🔬 Research Focus

Currently, I'm pioneering research in **physical-aware learning** for vision models and **large language model pretraining data analysis**. My work spans across multiple cutting-edge domains:

<div class="publication-card">

### 🌐 **Multimodal Foundation Models & World Models**
*My primary research passion and long-term vision*

I'm developing **native multimodal foundation models** capable of seamless **understanding**, **reasoning**, and **generation** across video, language, and speech modalities. These models serve as the computational "brain" for:
- 🤖 **Embodied AI Systems**
- 🤾 **Advanced Robotics**
- 🌍 **Interactive World Simulation**

</div>

<div class="publication-card">

### ⚡ **Reinforcement Learning & Adaptive Systems**

I investigate **reinforcement learning** methodologies built upon pretrained and SFT-initialized models, transcending traditional imitation learning to unlock:
- 🎯 **Novel Generative Capabilities**
- 🤖 **Autonomous Robotic Behaviors**  
- 🌐 **Agent Training in Learned World Environments**

</div>

<div class="publication-card">

### 📊 **Data-Centric Machine Learning**

My research encompasses comprehensive **training data analysis and optimization**:
- 🔍 **Data Quality Assessment & Enhancement**
- ⚡ **Efficient Data Compression for Scalable Training**
- 🏭 **Automated High-Quality Data Synthesis Pipelines**
- 📈 **Cost-Effective Data Curation Strategies**

</div>

</div>

<div class="research-section" data-parallax="0.1">

## 📰 Latest Updates

<div class="publication-card">

🚀 **[OpenCaptchaWorld](https://github.com/MetaAgentX/OpenCaptchaWorld)** has been released and expanded to double its original scope! This comprehensive web-based platform revolutionizes multimodal LLM agent testing and benchmarking.

</div>

</div>

<div class="research-section">

## 📚 Selected Publications
*( \* indicates equal contribution)*

> 📖 For my complete and up-to-date publication list, visit my [**Google Scholar**](https://scholar.google.com/citations?user=tEaSCzYAAAAJ&hl=en) profile.

<div class="publication-card">

### 🌐 **OpenCaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents**

**Venue:** *arXiv Preprint*  
**Authors:** **Yaxin Luo** \*, Zhaoyi Li \*, Jiacheng Liu, Jiacheng Cui, Xiaohan Zhao, Zhiqiang Shen

<img src="./images/opencaptchaworld.png" width="300px" align="right" style="margin-left:15px; border-radius: 12px; box-shadow: 0 6px 20px rgba(74,144,226,0.2);">

A groundbreaking platform that revolutionizes how we evaluate and benchmark multimodal large language model agents through comprehensive web-based testing environments.

**🔗 Resources:** [Paper](https://arxiv.org/abs/2505.24878) • [Code](https://github.com/MetaAgentX/OpenCaptchaWorld) • [Demo](https://huggingface.co/spaces/YaxinLuo/Open_CaptchaWorld)

</div>

<div class="publication-card">

### ⚓ **APL: Anchor-Based Prompt Learning for One-Stage Weakly Supervised Referring Expression Comprehension**

**Venue:** *ECCV 2024*  
**Authors:** **Yaxin Luo**, Jiayi Ji, Xiaofu Chen, Yuxin Zhang, Tianhe Ren, Gen Luo

<img src="./images/APL.png" width="300px" align="right" style="margin-left:15px; border-radius: 12px; box-shadow: 0 6px 20px rgba(74,144,226,0.2);">

Novel anchor-based prompt learning approach for efficiently understanding referring expressions in computer vision tasks without extensive supervision.

**🔗 Resources:** [Paper](https://link.springer.com/chapter/10.1007/978-3-031-72624-8_12) • [Code](https://github.com/Yaxin9Luo/APL)

</div>

<div class="publication-card">

### 🧠 **γ-MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models**

**Venue:** *ICLR 2025*  
**Authors:** **Yaxin Luo**, Gen Luo, Jiayi Ji, Yiyi Zhou, Xiaoshuai Sun, Zhiqiang Shen, Rongrong Ji

<img src="./images/MoD.png" width="300px" align="right" style="margin-left:15px; border-radius: 12px; box-shadow: 0 6px 20px rgba(74,144,226,0.2);">

Innovative mixture-of-depth architecture that enhances computational efficiency while maintaining performance in multimodal language models.

**🔗 Resources:** [Paper](https://arxiv.org/abs/2410.13859) • [Code](https://github.com/Yaxin9Luo/gamma-MoD)

</div>

<div class="publication-card">

### 🎯 **DViN: Dynamic Visual Routing Network for Weakly Supervised Referring Expression Comprehension**

**Venue:** *CVPR 2025*  
**Authors:** Xiaofu Chen, **Yaxin Luo**, Gen Luo, Jiayi Ji, Henghui Ding, Yiyi Zhou

<img src="./images/DViN.png" width="300px" align="right" style="margin-left:15px; border-radius: 12px; box-shadow: 0 6px 20px rgba(74,144,226,0.2);">

Dynamic visual routing mechanism that intelligently processes referring expressions through adaptive network pathways with minimal supervision.

**🔗 Resources:** [Paper](https://openaccess.thecvf.com/content/CVPR2025/html/Chen_DViN_Dynamic_Visual_Routing_Network_for_Weakly_Supervised_Referring_Expression_CVPR_2025_paper.html) • [Code](https://github.com/XxFChen/DViN)

</div>

</div>

<div class="research-section">

## 🚀 Let's Connect & Collaborate

I'm always excited to discuss cutting-edge AI research, explore potential collaborations, and share insights about the future of multimodal foundation models. Feel free to reach out!

**🔬 Research Interests Alignment?** • **💡 Innovative Ideas to Share?** • **🤝 Collaboration Opportunities?**

*Let's push the boundaries of AI together!*

</div>