---
permalink: /
title: "Hi there! üëã I am Yaxin Luo."
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<div class="section-header">
About Me
</div>

<div class="blob-field">
<span class="sr-only">Decorative animated background</span>
<div class="about-me-content">
  <div class="intro-text">
    Hello! I am a First-Year Machine Learning PhD student at <a href="https://mbzuai.ac.ae/" class="institution-link">MBZUAI</a>, advised by <a href="https://zhiqiangshen.com/" class="advisor-link">Prof. Zhiqiang Shen</a>, <a href="https://www.di.ens.fr/~laptev/" class="advisor-link">Prof. Ivan Laptev</a>  and <a href="https://fabvio.github.io/" class="advisor-link">Dr. Fabio Pizzati</a>. I am also closely working with my friend <a href="https://xxfchen.github.io/XiaofuChen/" class="collaborator-link">Xiaofu Chen</a>.
  </div>
  
  <div class="background-text">
    Previously, I received my Bachelor's degree from <a href="https://www.dtu.dk/english/" class="institution-link">Technical University of Denmark</a> and fortune to be supervised by <a href="https://dimipapa.github.io/" class="advisor-link">Prof. Dim P. Papadopoulos</a>.
  </div>
  
  <div class="current-focus">
    Recently, I am focusing on <strong>physical aware learning</strong> for vision models and <strong>analysis of pretrain data of LLM</strong>.
  </div>
  
  <div class="research-interests">
    <h4>My research interests span:</h4>
    <ul>
      <li>
        <strong> Multimodal Foundation Model </strong>: Developing native multimodal foundation models which can perform <strong>understanding</strong>, <strong>reasoning</strong>, <strong>generation</strong> tasks from video, language, speech. These models will serve as the core intelligence‚Äîthe "brain"‚Äîfor Embodied AI, Robotics, and many other applications. <em>(My Long-Term research interest and belief)</em>
      </li>
      
      <li>
        <strong> Video Understanding & Generation </strong>:  Vision-centric video model that learns causal structure and object-centric representations from large-scale video supports both understanding and generation with long-horizon temporal coherence; and further enabling action-conditioned prediction for embodied agents.
      </li>

      <li>
        <strong> Data-centric Machine Learning </strong>: Beyond the perspectives of models and algorithms, I am also enjoying to analysis and understand  data, improve data quality, compress data for training efficiency, and build efficient / scalable data pipelines for synthesizing high-quality  data for foundation models.
      </li>
    </ul>
  </div>
</div>
</div>


News
======

<div class="news-item">
üöÄ <strong><a href="https://github.com/MetaAgentX/OpenCaptchaWorld">OpenCaptchaWorld</a></strong> released and expanded to double the dataset size!
</div>


<div id="pub-strip-anchor"></div>

Selected Publications
======
*( * indicate equal contribution)*

For full and up-to-date publication list, please refer to my [Google Scholar](https://scholar.google.com/citations?user=tEaSCzYAAAAJ&hl=en) page.

<!-- Publications: reverted to original Markdown layout -->
* <img src="./images/opencaptchaworld.png" width="400px" align="left" style="margin-right:10px" class="publication-image"> **OpenCaptchaWorld: AComprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents**
  * <span class="venue-badge arxiv">arXiv</span>
  * **Yaxin Luo** *, Zhaoyi Li *, Jiacheng Liu, Jiacheng Cui, Xiaohan Zhao, Zhiqiang Shen
  * <a href="https://arxiv.org/abs/2505.24878" class="enhanced-link paper-link">üìÑ Paper</a> <a href="https://github.com/MetaAgentX/OpenCaptchaWorld" class="enhanced-link code-link">üíª Code</a> <a href="https://huggingface.co/spaces/YaxinLuo/Open_CaptchaWorld" class="enhanced-link demo-link">üöÄ Demo</a>

* <img src="./images/APL.png" width="400px" align="left" style="margin-right:10px" class="publication-image"> **APL: Anchor-Based Prompt Learning for One-Stage Weakly Supervised Referring Expression Comprehension**
  * <span class="venue-badge eccv">ECCV 2024</span>
  * **Yaxin Luo**,Jiayi Ji, Xiaofu Chen, Yuxin Zhang, Tianhe Ren, Gen Luo
  * <a href="https://link.springer.com/chapter/10.1007/978-3-031-72624-8_12" class="enhanced-link paper-link">üìÑ Paper</a> <a href="https://github.com/Yaxin9Luo/APL" class="enhanced-link code-link">üíª Code</a>

* <img src="./images/MoD.png" width="400px" align="left" style="margin-right:10px" class="publication-image"> **Œ≥-MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models**
  * <span class="venue-badge iclr">ICLR 2025</span>
  * **Yaxin Luo**, Gen Luo, Jiayi Ji, Yiyi Zhou, Xiaoshuai Sun, Zhiqiang Shen, Rongrong Ji
  * <a href="https://arxiv.org/abs/2410.13859" class="enhanced-link paper-link">üìÑ Paper</a> <a href="https://github.com/Yaxin9Luo/gamma-MoD" class="enhanced-link code-link">üíª Code</a>

* <img src="./images/DViN.png" width="400px" align="left" style="margin-right:10px" class="publication-image"> **DViN: Dynamic Visual Routing Network for Weakly Supervised Referring Expression Comprehension**
  * <span class="venue-badge cvpr">CVPR 2025</span>
  * Xiaofu Chen, **Yaxin Luo**, Gen Luo, Jiayi Ji, Henghui Ding, Yiyi Zhou
  * <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Chen_DViN_Dynamic_Visual_Routing_Network_for_Weakly_Supervised_Referring_Expression_CVPR_2025_paper.html" class="enhanced-link paper-link">üìÑ Paper</a> <a href="https://github.com/XxFChen/DViN" class="enhanced-link code-link">üíª Code</a>


<style>
/* Beautiful Conference Venue Badges */
.venue-badge {
  display: inline-block;
  padding: 6px 14px;
  border-radius: 20px;
  font-size: 0.9em;
  font-weight: 700;
  text-transform: uppercase;
  letter-spacing: 0.5px;
  color: white !important;
  margin: 4px 8px 4px 0;
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.15);
  transition: all 0.3s ease;
  position: relative;
  overflow: hidden;
}

.venue-badge:before {
  content: '';
  position: absolute;
  top: 0;
  left: -100%;
  width: 100%;
  height: 100%;
  background: linear-gradient(90deg, transparent, rgba(255,255,255,0.3), transparent);
  transition: left 0.5s;
}

.venue-badge:hover {
  transform: translateY(-2px) scale(1.05);
  box-shadow: 0 6px 20px rgba(0, 0, 0, 0.25);
}

.venue-badge:hover:before {
  left: 100%;
}

/* Specific Conference Colors */
.venue-badge.cvpr {
  background: linear-gradient(135deg, #e74c3c 0%, #c0392b 100%);
}
.venue-badge.cvpr:hover { box-shadow: 0 6px 20px rgba(231, 76, 60, 0.4); }

.venue-badge.eccv {
  background: linear-gradient(135deg, #9b59b6 0%, #8e44ad 100%);
}
.venue-badge.eccv:hover { box-shadow: 0 6px 20px rgba(155, 89, 182, 0.4); }

.venue-badge.iclr {
  background: linear-gradient(135deg, #3498db 0%, #2980b9 100%);
}
.venue-badge.iclr:hover { box-shadow: 0 6px 20px rgba(52, 152, 219, 0.4); }

.venue-badge.arxiv {
  background: linear-gradient(135deg, #f39c12 0%, #e67e22 100%);
}
.venue-badge.arxiv:hover { box-shadow: 0 6px 20px rgba(243, 156, 18, 0.4); }

.venue-badge.nips,
.venue-badge.neurips {
  background: linear-gradient(135deg, #1abc9c 0%, #16a085 100%);
}
.venue-badge.nips:hover,
.venue-badge.neurips:hover { box-shadow: 0 6px 20px rgba(26, 188, 156, 0.4); }

.venue-badge.icml {
  background: linear-gradient(135deg, #34495e 0%, #2c3e50 100%);
}
.venue-badge.icml:hover { box-shadow: 0 6px 20px rgba(52, 73, 94, 0.4); }

.venue-badge.aaai {
  background: linear-gradient(135deg, #e67e22 0%, #d35400 100%);
}
.venue-badge.aaai:hover { box-shadow: 0 6px 20px rgba(230, 126, 34, 0.4); }

.venue-badge.ijcai {
  background: linear-gradient(135deg, #27ae60 0%, #229954 100%);
}
.venue-badge.ijcai:hover { box-shadow: 0 6px 20px rgba(39, 174, 96, 0.4); }

/* News item enhanced styling */
.news-item {
  padding: 15px 20px;
  margin: 12px 0;
  background: linear-gradient(135deg, rgba(52, 152, 219, 0.1) 0%, rgba(155, 89, 182, 0.1) 100%);
  border-radius: 12px;
  border-left: 4px solid #3498db;
  transition: all 0.3s ease;
}
.news-item:hover {
  transform: translateX(5px);
  box-shadow: 0 4px 15px rgba(52, 152, 219, 0.2);
  background: linear-gradient(135deg, rgba(52, 152, 219, 0.15) 0%, rgba(155, 89, 182, 0.15) 100%);
}

/* About Me section enhancement */
.section-header {
  font-size: 1.5em;
  font-weight: 700;
  color: #2c3e50;
  margin: 30px 0 15px 0;
  padding-bottom: 8px;
  border-bottom: 3px solid #3498db;
  position: relative;
}
.section-header:after {
  content: '';
  position: absolute;
  bottom: -3px;
  left: 0;
  width: 50px;
  height: 3px;
  background: linear-gradient(90deg, #3498db, #9b59b6);
}

.about-me-content {
  padding: 20px;
  margin: 15px 0;
  background: linear-gradient(135deg, rgba(52, 152, 219, 0.1) 0%, rgba(155, 89, 182, 0.1) 100%);
  border-radius: 12px;
  border-left: 4px solid #3498db;
  transition: all 0.3s ease;
  line-height: 1.6;
}
.about-me-content:hover {
  transform: translateX(3px);
  box-shadow: 0 8px 25px rgba(52, 152, 219, 0.1);
}

.about-me-content ul { margin: 15px 0; padding-left: 20px; }
.about-me-content ul li {
  margin: 8px 0; padding: 8px 0; border-radius: 8px; transition: all 0.3s ease;
}
.about-me-content ul li:hover { background: rgba(52, 152, 219, 0.1); padding-left: 10px; }

/* ‚úÖ Âè™Âä†Á≤óÔºå‰∏ç‰∏äËâ≤ */
.about-me-content strong {
  color: inherit;
  font-weight: 700;
}

/* ÂÖ®Â±ÄÈìæÊé•ÔºàAbout Âå∫ÂüüÔºâ */
.about-me-content a {
  color: #3498db;
  text-decoration: none;
  transition: all 0.3s ease;
}
.about-me-content a:hover {
  color: #9b59b6;
  text-decoration: underline;
}

/* ‚úÖ Â≠¶Ê†°/Êú∫ÊûÑÔºöÂä†Á≤óÔºå‰øùÊåÅÁªßÊâøÈ¢úËâ≤ */
.about-me-content .institution-link {
  font-weight: 700;
  color: inherit;
  text-decoration: none;
  transition: all 0.2s ease;
}
.about-me-content .institution-link:hover {
  text-decoration: underline;
}

/* ‚úÖ ‰∫∫ÂêçÔºösoft blue-green */
.about-me-content .advisor-link,
.about-me-content .collaborator-link {
  color: #5dade2;
  font-weight: 600;
  text-decoration: none;
  transition: all 0.2s ease;
}
.about-me-content .advisor-link:hover,
.about-me-content .collaborator-link:hover {
  color: #3498db;
  text-decoration: underline;
}

.about-me-content .intro-text { margin-bottom: 20px; line-height: 1.7; font-size: 1.05em; }
.about-me-content .background-text { margin-bottom: 20px; line-height: 1.6; color: #555; }

.about-me-content .current-focus {
  margin-bottom: 25px; padding: 15px;
  background: rgba(173, 216, 230, 0.3);
  border-radius: 8px; border-left: 3px solid #87ceeb; font-weight: 500;
}

.about-me-content .research-interests { margin-top: 25px; }
.about-me-content .research-interests h4 { color: #2c3e50; margin-bottom: 15px; font-size: 1.1em; font-weight: 600; }
.about-me-content .research-interests ul { margin: 0; padding-left: 0; list-style: none; }
.about-me-content .research-interests li {
  margin: 15px 0; padding: 15px;
  background: rgba(173, 216, 230, 0.2);
  border-radius: 8px; border-left: 3px solid #87ceeb;
  transition: all 0.3s ease; line-height: 1.6;
}
.about-me-content .research-interests li:hover {
  background: rgba(173, 216, 230, 0.3);
  transform: translateX(5px);
  box-shadow: 0 4px 15px rgba(135, 206, 235, 0.2);
}

.about-me-content em { color: #7f8c8d; font-style: italic; }

/* Greeting Title (unchanged visual flair) */
.greeting-title {
  display: inline-block;
  background: linear-gradient(135deg, #667eea 0%, #764ba2 50%, #f093fb 100%);
  background-size: 200% 200%;
  -webkit-background-clip: text;
  -webkit-text-fill-color: transparent;
  background-clip: text;
  animation: gradient-shift 3s ease infinite;
  font-weight: 700;
  position: relative;
  box-shadow: 0 0 20px rgba(102, 126, 234, 0.3);
  transition: all 0.3s ease;
}
.greeting-title:hover { transform: scale(1.02); box-shadow: 0 0 30px rgba(102, 126, 234, 0.5); }
.greeting-title:before {
  content: ''; position: absolute; top: -2px; left: -2px; right: -2px; bottom: -2px;
  background: linear-gradient(135deg, #667eea, #764ba2, #f093fb);
  background-size: 200% 200%; animation: gradient-shift 3s ease infinite; z-index: -1;
  border-radius: 8px; opacity: 0.3; filter: blur(4px);
}
.greeting-title:after { content: '‚ú®'; position: absolute; top: -10px; right: -15px; font-size: 0.8em; animation: sparkle 2s ease-in-out infinite; opacity: 0.8; }

.wave-emoji { display: inline-block; animation: wave 2s ease-in-out infinite; transform-origin: 70% 70%; font-size: 1.2em; }
@keyframes wave { 0%,100%{transform:rotate(0)}10%{transform:rotate(14deg)}20%{transform:rotate(-8deg)}30%{transform:rotate(14deg)}40%{transform:rotate(-4deg)}50%{transform:rotate(10deg)}60%{transform:rotate(0)} }
@keyframes rainbow-shift { 0%{background-position:0% 50%}50%{background-position:100% 50%}100%{background-position:0% 50%} }
@keyframes sparkle { 0%,100%{transform:scale(1) rotate(0); opacity:.8}50%{transform:scale(1.2) rotate(180deg); opacity:1} }

/* Responsive design for badges */
@media (max-width: 768px) {
  .venue-badge { padding: 4px 10px; font-size: 0.8em; margin: 2px 4px 2px 0; }
}
@media (max-width: 480px) {
  .venue-badge { padding: 3px 8px; font-size: 0.75em; }
}
</style>
