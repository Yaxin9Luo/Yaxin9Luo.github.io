---
permalink: /
title: "Hi there! 👋 I am Yaxin Luo."
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

About Me
======
Hello! I am a First-Year Machine Learning PhD student at [MBZUAI](https://mbzuai.ac.ae/), advised by [Prof. Zhiqiang Shen](https://zhiqiangshen.com/), [Dr.Fabio Pizzati](https://fabvio.github.io/) and [Prof.Ivan Laptev](https://www.di.ens.fr/~laptev/) . I am also closely working with my friend [Xiaofu Chen](https://xxfchen.github.io/XiaofuChen/). Previously, I was a Research Assistant at MBZUAI and received my Bachelor's degree from [Technical University of Denmark](https://www.dtu.dk/english/) supervised by [Prof. Dim P. Papadopoulos](https://dimipapa.github.io/). Recently, I am focusing on **physical aware learning** for vision models and **analysis the pretrain data of LLM**. My research interests span in :
- **Multimodal Foundation Model / World Model**: Developing native multimodal foundation models which can perform  **understanding**, **reasoning**, **generation** tasks from video,language, speech. These models will serve as the core intelligence—the "brain"—for Embodied AI, Robotics, and many other applications. **(My Long-Term research interest and belief)**

- **Reinforcement Learning**: I study reinforcement learning on top of pretrained and SFT-initialized models to move beyond imitation—unlocking new capabilities in generative modeling and robotics, including training agents inside learned world-model environments.

- **Data-centric Machine Learning**: Analysis and Understanding the training data of the model, improve the data quality, compress the data for training efficiency, scalable and cheap data pipline for curating or synthesis high quality training data.

## 🛠️ Technical Skills

<div class="skills-container">
  <div class="skill-category">
    <h4>🤖 Machine Learning</h4>
    <div class="skill-tags">
      <span class="skill-tag ml">PyTorch</span>
      <span class="skill-tag ml">TensorFlow</span>
      <span class="skill-tag ml">Transformers</span>
      <span class="skill-tag ml">CUDA</span>
      <span class="skill-tag ml">MLflow</span>
    </div>
  </div>
  
  <div class="skill-category">
    <h4>💻 Programming</h4>
    <div class="skill-tags">
      <span class="skill-tag prog">Python</span>
      <span class="skill-tag prog">C++</span>
      <span class="skill-tag prog">JavaScript</span>
      <span class="skill-tag prog">MATLAB</span>
      <span class="skill-tag prog">R</span>
    </div>
  </div>
  
  <div class="skill-category">
    <h4>🌐 Tools & Platforms</h4>
    <div class="skill-tags">
      <span class="skill-tag tools">Docker</span>
      <span class="skill-tag tools">AWS</span>
      <span class="skill-tag tools">Git</span>
      <span class="skill-tag tools">Linux</span>
      <span class="skill-tag tools">Slurm</span>
    </div>
  </div>
</div>

<style>
.skills-container {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
  gap: 20px;
  margin: 20px 0;
}

.skill-category {
  background: linear-gradient(135deg, rgba(102, 126, 234, 0.05) 0%, rgba(118, 75, 162, 0.05) 100%);
  padding: 20px;
  border-radius: 12px;
  border-left: 4px solid #667eea;
}

.skill-category h4 {
  margin: 0 0 15px 0;
  color: #667eea;
  font-weight: 600;
}

.skill-tags {
  display: flex;
  flex-wrap: wrap;
  gap: 8px;
}

.skill-tag {
  padding: 6px 12px;
  border-radius: 20px;
  font-size: 0.85em;
  font-weight: 500;
  color: white;
  transition: all 0.3s ease;
  cursor: pointer;
}

.skill-tag:hover {
  transform: translateY(-2px) scale(1.05);
  box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2);
}

.skill-tag.ml {
  background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);
}

.skill-tag.prog {
  background: linear-gradient(135deg, #43e97b 0%, #38f9d7 100%);
}

.skill-tag.tools {
  background: linear-gradient(135deg, #fa709a 0%, #fee140 100%);
}
</style> 

News
======

-- 🚀 <strong>[OpenCaptchaWorld](https://github.com/MetaAgentX/OpenCaptchaWorld)</strong> released and expanded to double the dataset size!



Selected Publications
======
*( * indicate equal contribution)*

For full and up-to-date publication list, please refer to my [Google Scholar](https://scholar.google.com/citations?user=tEaSCzYAAAAJ&hl=en) page.

* <img src="./images/opencaptchaworld.png" width="400px" align="left" style="margin-right:10px" class="publication-image"> **OpenCaptchaWorld: AComprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents**
  * Arxiv
  * **Yaxin Luo** *, Zhaoyi Li *, Jiacheng Liu, Jiacheng Cui, Xiaohan Zhao, Zhiqiang Shen
  * <a href="https://arxiv.org/abs/2505.24878" class="enhanced-link paper-link">📄 Paper</a> <a href="https://github.com/MetaAgentX/OpenCaptchaWorld" class="enhanced-link code-link">💻 Code</a> <a href="https://huggingface.co/spaces/YaxinLuo/Open_CaptchaWorld" class="enhanced-link demo-link">🚀 Demo</a>

* <img src="./images/APL.png" width="400px" align="left" style="margin-right:10px" class="publication-image"> **APL: Anchor-Based Prompt Learning for One-Stage Weakly Supervised Referring Expression Comprehension**
  * ECCV 2024
  * **Yaxin Luo**,Jiayi Ji, Xiaofu Chen, Yuxin Zhang, Tianhe Ren, Gen Luo
  * <a href="https://link.springer.com/chapter/10.1007/978-3-031-72624-8_12" class="enhanced-link paper-link">📄 Paper</a> <a href="https://github.com/Yaxin9Luo/APL" class="enhanced-link code-link">💻 Code</a>

* <img src="./images/MoD.png" width="400px" align="left" style="margin-right:10px" class="publication-image"> **γ-MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models**
  * ICLR 2025
  * **Yaxin Luo**, Gen Luo, Jiayi Ji, Yiyi Zhou, Xiaoshuai Sun, Zhiqiang Shen, Rongrong Ji
  * <a href="https://arxiv.org/abs/2410.13859" class="enhanced-link paper-link">📄 Paper</a> <a href="https://github.com/Yaxin9Luo/gamma-MoD" class="enhanced-link code-link">💻 Code</a>

* <img src="./images/DViN.png" width="400px" align="left" style="margin-right:10px" class="publication-image"> **DViN: Dynamic Visual Routing Network for Weakly Supervised Referring Expression Comprehension**
  * CVPR 2025
  * Xiaofu Chen, **Yaxin Luo**, Gen Luo, Jiayi Ji, Henghui Ding, Yiyi Zhou
  * <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Chen_DViN_Dynamic_Visual_Routing_Network_for_Weakly_Supervised_Referring_Expression_CVPR_2025_paper.html" class="enhanced-link paper-link">📄 Paper</a> <a href="https://github.com/XxFChen/DViN" class="enhanced-link code-link">💻 Code</a>