---
permalink: /
title: "Hi there! ðŸ‘‹ I am Yaxin Luo."
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

About Me
======
Hello! I am a First-Year Machine Learning PhD student at [MBZUAI](https://mbzuai.ac.ae/), advising by [Prof. Zhiqiang Shen](https://scholar.google.com/citations?user=DGr0fVoAAAAJ&hl=en). I am also closely working with my friend [Xiaofu Chen](https://scholar.google.com/citations?user=dks3OoUAAAAJ&hl=en). Previously, I was a Research Assistant at [MBZUAI](https://mbzuai.ac.ae/) and received my Bachelor's degree from [Technical University of Denmark](https://www.dtu.dk/english/) supervised by [Prof. Dim P. Papadopoulos](https://scholar.google.com/citations?user=-_JAhdQAAAAJ&hl=en). My research interests span in :
- **Multimodal Foundation Model / World Model**: Developing native multimodal foundation models which can perform  **understanding**, **reasoning**, **generation** tasks from video,language, speech. These models will serve as the core intelligenceâ€”the "brain"â€”for Embodied AI, Robotics, and many other applications. **(My Long-Term research interest and belief)**
- **Data Synthesis**: Foundation Models are data consuming monsters, the quantity and quality of real-world data is way more behind to train a multimodal foundation model or world model, using advanced generative models to create vast quantities of diverse, high-quality training data can accelerates the development of this progress. **(Support the first goal)**
- **Efficient deep learning**: Improving training and inference efficiency of large foundation models from data, system and model perspectives, invoking real-world services. **(Support the first goal)**

News
======
- [2025-05-16] **DRAG** is accepted by ACL 2025 main conference!!
- [2025-01-22] **Î³-MoD** is accepted by ICLR 2025, see you in Singapore!

Selected Publications
======
*( * indicate equal contribution)*

For full and up-to-date publication list, please refer to my [Google Scholar](https://scholar.google.com/citations?user=tEaSCzYAAAAJ&hl=en) page.

* <img src="./images/opencaptchaworld.png" width="400px" align="left" style="margin-right:10px"> **OpenCaptchaWorld: AComprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents**
  * Arxiv
  * Yaxin Luo, Zhaoyi Li, Jiacheng Liu, Jiacheng Cui, Xiaohan Zhao, Zhiqiang Shen
  * [Paper](https://arxiv.org/abs/2505.24878) | [Code](https://github.com/MetaAgentX/OpenCaptchaWorld) | [Demo](https://huggingface.co/spaces/YaxinLuo/Open_CaptchaWorld)

* <img src="./images/APL.png" width="400px" align="left" style="margin-right:10px"> **APL: Anchor-Based Prompt Learning for One-Stage Weakly Supervised Referring Expression Comprehension**
  * ECCV 2024
  * Yaxin Luo,Jiayi Ji, Xiaofu Chen, Yuxin Zhang, Tianhe Ren, Gen Luo
  * [Paper](https://link.springer.com/chapter/10.1007/978-3-031-72624-8_12) | [Code](https://github.com/Yaxin9Luo/APL)

* <img src="./images/MoD.png" width="400px" align="left" style="margin-right:10px"> **Î³-MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models**
  * ICLR 2025
  * Yaxin Luo, Gen Luo, Jiayi Ji, Yiyi Zhou, Xiaoshuai Sun, Zhiqiang Shen, Rongrong Ji
  * [Paper](https://arxiv.org/abs/2410.13859) | [Code](https://github.com/Yaxin9Luo/gamma-MoD)

* <img src="./images/DViN.png" width="400px" align="left" style="margin-right:10px"> **DViN: Dynamic Visual Routing Network for Weakly Supervised Referring Expression Comprehension**
  * CVPR 2025
  * Xiaofu Chen, Yaxin Luo, Gen Luo, Jiayi Ji, Henghui Ding, Yiyi Zhou
  * [Paper](https://openaccess.thecvf.com/content/CVPR2025/html/Chen_DViN_Dynamic_Visual_Routing_Network_for_Weakly_Supervised_Referring_Expression_CVPR_2025_paper.html) | [Code](https://github.com/XxFChen/DViN)