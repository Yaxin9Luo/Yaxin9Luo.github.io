---
permalink: /
title: "ğŸ¤– Yaxin Luo - AI Researcher"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
classes: wide tech-decoration
---

<div class="research-section" data-parallax="0.3">

## ğŸ§  About Me

Greetings! I'm a **First-Year Machine Learning PhD student** at [**MBZUAI**](https://mbzuai.ac.ae/), where I'm privileged to work under the guidance of an exceptional advisory team: [**Prof. Zhiqiang Shen**](https://zhiqiangshen.com/), [**Dr. Fabio Pizzati**](https://fabvio.github.io/), and [**Prof. Ivan Laptev**](https://www.di.ens.fr/~laptev/). I'm also collaborating closely with my research partner [**Xiaofu Chen**](https://xxfchen.github.io/XiaofuChen/).

My academic journey began with a **Bachelor's degree from [Technical University of Denmark](https://www.dtu.dk/english/)**, where I was mentored by [**Prof. Dim P. Papadopoulos**](https://dimipapa.github.io/). Prior to my PhD, I gained valuable experience as a **Research Assistant at MBZUAI**.

</div>

<div class="research-section" data-parallax="0.2">

## ğŸ”¬ Research Focus

Currently, I'm pioneering research in **physical-aware learning** for vision models and **large language model pretraining data analysis**. My work spans across multiple cutting-edge domains:

<div class="publication-card">

### ğŸŒ **Multimodal Foundation Models & World Models**
*My primary research passion and long-term vision*

I'm developing **native multimodal foundation models** capable of seamless **understanding**, **reasoning**, and **generation** across video, language, and speech modalities. These models serve as the computational "brain" for:
- ğŸ¤– **Embodied AI Systems**
- ğŸ¤¾ **Advanced Robotics**
- ğŸŒ **Interactive World Simulation**

</div>

<div class="publication-card">

### âš¡ **Reinforcement Learning & Adaptive Systems**

I investigate **reinforcement learning** methodologies built upon pretrained and SFT-initialized models, transcending traditional imitation learning to unlock:
- ğŸ¯ **Novel Generative Capabilities**
- ğŸ¤– **Autonomous Robotic Behaviors**  
- ğŸŒ **Agent Training in Learned World Environments**

</div>

<div class="publication-card">

### ğŸ“Š **Data-Centric Machine Learning**

My research encompasses comprehensive **training data analysis and optimization**:
- ğŸ” **Data Quality Assessment & Enhancement**
- âš¡ **Efficient Data Compression for Scalable Training**
- ğŸ­ **Automated High-Quality Data Synthesis Pipelines**
- ğŸ“ˆ **Cost-Effective Data Curation Strategies**

</div>

</div>

<div class="research-section" data-parallax="0.1">

## ğŸ“° Latest Updates

<div class="publication-card">

ğŸš€ **[OpenCaptchaWorld](https://github.com/MetaAgentX/OpenCaptchaWorld)** has been released and expanded to double its original scope! This comprehensive web-based platform revolutionizes multimodal LLM agent testing and benchmarking.

</div>

</div>

<div class="research-section">

## ğŸ“š Selected Publications
*( \* indicates equal contribution)*

> ğŸ“– For my complete and up-to-date publication list, visit my [**Google Scholar**](https://scholar.google.com/citations?user=tEaSCzYAAAAJ&hl=en) profile.

<div class="publication-card">

### ğŸŒ **OpenCaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents**

**Venue:** *arXiv Preprint*  
**Authors:** **Yaxin Luo** \*, Zhaoyi Li \*, Jiacheng Liu, Jiacheng Cui, Xiaohan Zhao, Zhiqiang Shen

<img src="./images/opencaptchaworld.png" width="300px" align="right" style="margin-left:15px; border-radius: 12px; box-shadow: 0 6px 20px rgba(74,144,226,0.2);">

A groundbreaking platform that revolutionizes how we evaluate and benchmark multimodal large language model agents through comprehensive web-based testing environments.

**ğŸ”— Resources:** [Paper](https://arxiv.org/abs/2505.24878) â€¢ [Code](https://github.com/MetaAgentX/OpenCaptchaWorld) â€¢ [Demo](https://huggingface.co/spaces/YaxinLuo/Open_CaptchaWorld)

</div>

<div class="publication-card">

### âš“ **APL: Anchor-Based Prompt Learning for One-Stage Weakly Supervised Referring Expression Comprehension**

**Venue:** *ECCV 2024*  
**Authors:** **Yaxin Luo**, Jiayi Ji, Xiaofu Chen, Yuxin Zhang, Tianhe Ren, Gen Luo

<img src="./images/APL.png" width="300px" align="right" style="margin-left:15px; border-radius: 12px; box-shadow: 0 6px 20px rgba(74,144,226,0.2);">

Novel anchor-based prompt learning approach for efficiently understanding referring expressions in computer vision tasks without extensive supervision.

**ğŸ”— Resources:** [Paper](https://link.springer.com/chapter/10.1007/978-3-031-72624-8_12) â€¢ [Code](https://github.com/Yaxin9Luo/APL)

</div>

<div class="publication-card">

### ğŸ§  **Î³-MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models**

**Venue:** *ICLR 2025*  
**Authors:** **Yaxin Luo**, Gen Luo, Jiayi Ji, Yiyi Zhou, Xiaoshuai Sun, Zhiqiang Shen, Rongrong Ji

<img src="./images/MoD.png" width="300px" align="right" style="margin-left:15px; border-radius: 12px; box-shadow: 0 6px 20px rgba(74,144,226,0.2);">

Innovative mixture-of-depth architecture that enhances computational efficiency while maintaining performance in multimodal language models.

**ğŸ”— Resources:** [Paper](https://arxiv.org/abs/2410.13859) â€¢ [Code](https://github.com/Yaxin9Luo/gamma-MoD)

</div>

<div class="publication-card">

### ğŸ¯ **DViN: Dynamic Visual Routing Network for Weakly Supervised Referring Expression Comprehension**

**Venue:** *CVPR 2025*  
**Authors:** Xiaofu Chen, **Yaxin Luo**, Gen Luo, Jiayi Ji, Henghui Ding, Yiyi Zhou

<img src="./images/DViN.png" width="300px" align="right" style="margin-left:15px; border-radius: 12px; box-shadow: 0 6px 20px rgba(74,144,226,0.2);">

Dynamic visual routing mechanism that intelligently processes referring expressions through adaptive network pathways with minimal supervision.

**ğŸ”— Resources:** [Paper](https://openaccess.thecvf.com/content/CVPR2025/html/Chen_DViN_Dynamic_Visual_Routing_Network_for_Weakly_Supervised_Referring_Expression_CVPR_2025_paper.html) â€¢ [Code](https://github.com/XxFChen/DViN)

</div>

</div>

<div class="research-section">

## ğŸš€ Let's Connect & Collaborate

I'm always excited to discuss cutting-edge AI research, explore potential collaborations, and share insights about the future of multimodal foundation models. Feel free to reach out!

**ğŸ”¬ Research Interests Alignment?** â€¢ **ğŸ’¡ Innovative Ideas to Share?** â€¢ **ğŸ¤ Collaboration Opportunities?**

*Let's push the boundaries of AI together!*

</div>