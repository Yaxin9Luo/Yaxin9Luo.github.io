---
title: "γ-MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models"
collection: publications
category: manuscripts
permalink: /publication/gamma-MoD
excerpt: 'This paper proposes γ-MoD, an innovative MoD adaptation strategy for existing MLLMs to reduce computational costs while maintaining performance.'
date: 2024-10-17
venue: 'arXiv preprint'
paperurl: 'https://arxiv.org/abs/2410.13859'
citation: 'Luo, Y., Luo, G., Ji, J., Zhou, Y., Sun, X., Shen, Z., & Ji, R. (2024). &quot;γ-MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models.&quot; <i>arXiv preprint arXiv:2410.13859</i>.'
---

This paper introduces γ-MoD, a novel approach to optimize MLLMs efficiency. With only -1.5% performance drop, γ-MoD reduces LLaVA-HR's training and inference time by 31.0% and 53.2% respectively through innovative MoD adaptation strategies.